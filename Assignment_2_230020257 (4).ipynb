{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 3: Geopandas and Rasterio**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IrSS9pONE8uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Loading the Data\n",
        "\n",
        " I mounted my google drive to my google colab"
      ],
      "metadata": {
        "id": "l5yi9xyLFpFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # mount the content from google drive into the google colab notebook"
      ],
      "metadata": {
        "id": "_H5Mp3K4F7RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Importing Libraries\n",
        "\n",
        "I imported the necessary python libraries to aid my analysis"
      ],
      "metadata": {
        "id": "ADHy83grGhl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contextily # Install contextily library\n"
      ],
      "metadata": {
        "id": "QqrBOkswGzUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mapclassify # Install mapclassify library"
      ],
      "metadata": {
        "id": "kCLVg60_Hdth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries for analysis\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import geopandas as gpd\n",
        "import contextily as ctx\n",
        "import rasterio as rio\n",
        "from rasterio import plot\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 8]"
      ],
      "metadata": {
        "id": "s7RyVr3PHmRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Reading in the data\n",
        "\n",
        "I read in the data using geopandas"
      ],
      "metadata": {
        "id": "H19-v2E7GRrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf = gpd.read_file(\"/content/drive/My Drive/Census20_LSOA.shp\") # loads in the shapefile data using geopandas\n",
        "gdf.head()  # shows 1st 5 rows of the dataset"
      ],
      "metadata": {
        "id": "cIBhDM4aGWjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Subsetting\n",
        "\n",
        "I subsetted the following columns, and geometry\n",
        "\n",
        "LSOA11CD --> LSOA area code LSOA11NM --> LSOA area name LSOA11NMW --> LSOA bigger area Pop20 --> Population counts"
      ],
      "metadata": {
        "id": "OGV3c5mUHybR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset = gdf[[\"LSOA11CD\",\"LSOA11NM\",\"LSOA11NMW\",\"Pop20\",\"geometry\"]]  # subsets these columns for analysis and adds the geometries\n",
        "subset.head() #shows 1st 5 rows of dataset"
      ],
      "metadata": {
        "id": "YuV5SvE1IHvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Find CRS\n",
        "\n",
        "I identified the coordinate reference system of the layer"
      ],
      "metadata": {
        "id": "Cm5xH-WfIUnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gdf.crs) # shows coordinate reference system of layer"
      ],
      "metadata": {
        "id": "-IUzlVC1Ieq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: How many Features\n",
        "\n",
        "I identified the number of features in the layer and expressed the results as an integer"
      ],
      "metadata": {
        "id": "M96tOL9WIiJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_numbers = int(len(gdf))  # counts number of features in dataframe\n",
        "print(feature_numbers)  # prints results"
      ],
      "metadata": {
        "id": "kTUCskUIIzsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Check for Duplicates\n",
        "\n",
        "I made sure that the values in the \"LSOA11CD\" column were unique by using pandas to check for duplicates"
      ],
      "metadata": {
        "id": "YLHakRhxI5Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = gdf.duplicated('LSOA11CD')  # checks for duplicates in the \"LSOA11CD\" column\n",
        "print(duplicates)  # prints results (true or false)"
      ],
      "metadata": {
        "id": "weNMzrr8JXrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Plotting\n",
        "\n",
        "I plot the layer using the .plot method"
      ],
      "metadata": {
        "id": "wLJEk_7iJcCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.plot()  # Plots dataset for visualisation"
      ],
      "metadata": {
        "id": "F7k4iY0iJtWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Plotting 2.0\n",
        "\n",
        "I plot the layer using the .explorer method"
      ],
      "metadata": {
        "id": "SNk_iMjeJ1V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.explore()  # maps dataset in dynamic manner"
      ],
      "metadata": {
        "id": "4N0LSTOXKC70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Subsetting\n",
        "\n",
        "I subset just the LSOA areas with Pop20 counts greater than 1500."
      ],
      "metadata": {
        "id": "vhYpLFSmKM3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_subset = gdf.mask(gdf[\"Pop20\"] <= 1500).dropna(subset=[\"Pop20\"])  # removes population counts the same as or lower than 1500\n",
        "print(population_subset)  # prints results"
      ],
      "metadata": {
        "id": "ICylkcFnKfaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11: Plotting the subset\n",
        "\n",
        "I plotted the resulting subset, using symbology according to total population size, i.e., the \"Pop_Total\" column, and using a sequentual color map such as \"Reds\""
      ],
      "metadata": {
        "id": "TwSHDBVuLDIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.plot(column=\"Pop20\", cmap=\"Reds\", legend=True, scheme=\"quantiles\", figsize=(10, 8))  # plots population data in red, using qualtile scheme"
      ],
      "metadata": {
        "id": "aksCkTxmLRkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12: How many Areas\n",
        "\n",
        "I identified how many areas there were in the requested population using the shape function"
      ],
      "metadata": {
        "id": "nWBrGZL0LUd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(population_subset.shape[0])  # shows the number of areas in the subset"
      ],
      "metadata": {
        "id": "H0_wY8HbLk49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13: What's the Population\n",
        "\n",
        "I identified the total population of the subset layer using the sum function"
      ],
      "metadata": {
        "id": "R6DXrfqMLqF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_pop = population_subset[\"Pop20\"].sum()  # calculates total population in the population subset\n",
        "print(total_pop) # prints results"
      ],
      "metadata": {
        "id": "y4-AOzf9L5RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rasterio** **Exercises**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NlPxBX0EL9Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Import Rasterio\n",
        "\n",
        "I imported the python library rasterio to aid my analysis"
      ],
      "metadata": {
        "id": "mJjM7sB3MSXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio as rio  # imports library"
      ],
      "metadata": {
        "id": "GSnmwmQuMnBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Reading in the file\n",
        "\n",
        "I read in the file as a rasterio dataset using rio.open"
      ],
      "metadata": {
        "id": "0daH9KVrMr4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = rio.open(\"/content/drive/My Drive/Clipped_Raster.tif\")  # loads the tif file from google drive"
      ],
      "metadata": {
        "id": "86XVkXTLM-N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Check CRS\n",
        "\n",
        "I identified the crs of the dataset"
      ],
      "metadata": {
        "id": "4VMFO6N6NCnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.crs)  # shows coordinate reference system of the dataset"
      ],
      "metadata": {
        "id": "yNSOoWItNJcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Extent\n",
        "\n",
        "I identified the raster extent of the dataset in projected coordinates"
      ],
      "metadata": {
        "id": "pvrLQM4kNN2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.bounds)  # shows extent of the dataset"
      ],
      "metadata": {
        "id": "VhvRBYl9Ne2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: How many Bands\n",
        "\n",
        "I identified how many bands there were in the dataset"
      ],
      "metadata": {
        "id": "dN4gGvBrNioP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.count)  # shows number of bands in dataset"
      ],
      "metadata": {
        "id": "MIH-kwd1NrkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Plot\n",
        "\n",
        "I created a plot of the image"
      ],
      "metadata": {
        "id": "kT1vLuIqNzoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [10,8]  # input figure parameters\n",
        "plt.imshow(dataset.read(1), cmap='gray')  # shows raster image in greyscale"
      ],
      "metadata": {
        "id": "a99ZxM3AOBIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Histogram\n",
        "\n",
        "I created a histogram from the raster"
      ],
      "metadata": {
        "id": "bGawomBiOMF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(dataset.read(1).flatten(), bins='auto')  # creates histogram of raster with automatic bin sizing"
      ],
      "metadata": {
        "id": "maVyhaiLOTvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: False colour plot\n",
        "\n",
        "I created a false colour plot of the raster using the python library EarthPy"
      ],
      "metadata": {
        "id": "vmHUWD0yOZ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "band1 = dataset.read(1)\n",
        "ep.plot_rgb(band1,\n",
        "            rgb=[3, 2, 1],\n",
        "            stretch=True,\n",
        "            str_clip=10)\n",
        "plt.show()                     # This code failed to run because false colour plots require multiple bands and this dataset only has 1 band"
      ],
      "metadata": {
        "id": "qBBhJMyJOx9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 4: Spatial Clustering (k-means and DBSCAN)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lvb-2j7WQHVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A: Data Exploration and Pre-Proccessing**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VCFzZ7gdcUgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Loading in the data\n",
        "\n",
        "I mounted my google drive to my google colab"
      ],
      "metadata": {
        "id": "Q7WsaTnvSuL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # mounts content from google drive to google colab notebook"
      ],
      "metadata": {
        "id": "hwxSseXoVotW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Importing Libraries\n",
        "\n",
        "I imported the necessary puthon libraries to aid my analysis"
      ],
      "metadata": {
        "id": "75SVtdv-Vsve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lonboard  # imports library"
      ],
      "metadata": {
        "id": "PA7gaVrQV3D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports necessary library for analysis\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import shapely\n",
        "import folium\n",
        "import seaborn as sns\n",
        "from lonboard import Map, ScatterplotLayer, viz"
      ],
      "metadata": {
        "id": "6x5UPydpWScW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Reading in the data\n",
        "I used pandas to load in the car accidents data"
      ],
      "metadata": {
        "id": "CBrlbs3dPCcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_accidents = pd.read_csv('/content/drive/MyDrive/UK_Accident.csv')  # loads csv file from google drive"
      ],
      "metadata": {
        "id": "jdRYUh4eW13X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Display top of dataset\n",
        "\n",
        "I I displayed the 1st few rows of the dataset to see the available attributes"
      ],
      "metadata": {
        "id": "r_me8bRBXAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(car_accidents.head())  # shows 1st 5 rows of the dataset"
      ],
      "metadata": {
        "id": "KpZZ4Py6XS3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Necessary Columns\n",
        "\n",
        "I kept the necessary columns from the dataset so that the datset had less noise"
      ],
      "metadata": {
        "id": "NMfhHfVXX27A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "necessary_columns = [\"Accident_Severity\", \"Number_of_Vehicles\", \"Number_of_Casualties\", \"Speed_limit\", \"Day_of_Week\", \"Road_Type\", \"Light_Conditions\", \"Weather_Conditions\", \"Road_Surface_Conditions\", \"Urban_or_Rural_Area\", \"Year\", \"Latitude\", \"Longitude\"]\n",
        "car_accidents = car_accidents[necessary_columns]  # selects necessary columns into a subset of the data\n",
        "print(car_accidents.head())  # we can see the change now"
      ],
      "metadata": {
        "id": "STPoq272YFZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Slicing the Dataframe\n",
        "\n",
        "I sliced the pandas dataframe until I was left with only 2010 records in order to specify and reduce the analysis"
      ],
      "metadata": {
        "id": "zS6scWWVYN_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_accidents_2010 = car_accidents[car_accidents[\"Year\"] == 2010]  # slices dataframe into a subset with only 2010 records\n",
        "print(car_accidents_2010.head())  # we can see the change now"
      ],
      "metadata": {
        "id": "AYoQX02gYhQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Plotting\n",
        "\n",
        "I used the dataset to make a plot showing which day of the week had the most car accidents"
      ],
      "metadata": {
        "id": "3Jh3kqbVYomY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "day_num = {\n",
        "    1: \"Sunday\",\n",
        "    2: \"Monday\",\n",
        "    3: \"Tuesday\",\n",
        "    4: \"Wednesday\",\n",
        "    5: \"Thursday\",\n",
        "    6: \"Friday\",\n",
        "    7: \"Saturday\"  # turn numerical values into days for the plot\n",
        "}\n",
        "car_accidents_2010[\"Day_of_Week\"] = car_accidents_2010[\"Day_of_Week\"].map(day_num)\n",
        "accidents_per_day = car_accidents_2010[\"Day_of_Week\"].value_counts()  # counts accidents per day\n",
        "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]  # creates the order for the graph\n",
        "accidents_per_day = accidents_per_day.reindex(day_order)  # reorders from monday to Sunday\n",
        "accidents_per_day.plot(kind=\"bar\")  # plots the graph"
      ],
      "metadata": {
        "id": "AwqtY83MY7aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Plotting a Relationship\n",
        "\n",
        "I made a plot to explore the relationship between Accident Severity and Road Conditions"
      ],
      "metadata": {
        "id": "-cB-N5vSZBwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accident_severity_road_condition = pd.crosstab(  # creates contingency table for attributes\n",
        "    car_accidents_2010[\"Accident_Severity\"],\n",
        "    car_accidents_2010[\"Road_Surface_Conditions\"]\n",
        ")\n",
        "accident_severity_road_condition.plot(kind=\"bar\")  # graphs contingency table as bar graph showing relationship"
      ],
      "metadata": {
        "id": "q0XqLbr_ZST4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that wet/damp roads make up a good chunk of the accidents within severity level 3. However severity level 3 is dominated by dry road conditions. Intuitively this might seem unusual but once you realise that dry roads are most commonly driven on, it makes more sense. In this instance the graph casn be misleading as it's not ratiod down with other road conditions"
      ],
      "metadata": {
        "id": "Dz7wNgEVZfzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Mappin car accidents\n",
        "\n",
        "I used the lonboard python library to map all the car accidents within my filtered 2010 dataset"
      ],
      "metadata": {
        "id": "61WwI1oIbAvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.artist import get\n",
        "import lonboard as lb  # imports libraries\n",
        "\n",
        "geometry = gpd.points_from_xy(car_accidents_2010[\"Longitude\"], car_accidents_2010[\"Latitude\"])  # changes latitude and longitude into points\n",
        "\n",
        "gdf_accidents = gpd.GeoDataFrame(car_accidents_2010, geometry=geometry, crs=\"EPSG:4326\")  # geo data frame made from geometry points and using \"EPSG:4326\" as the CRS\n",
        "\n",
        "\n",
        "\n",
        "lb.viz(gdf_accidents)  # maps the points for accidents"
      ],
      "metadata": {
        "id": "N7tGZj8TbOwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Spatial Filtering\n",
        "\n",
        "I I created a new dataset  to map only the car accidents in the Glasgow-Edinburgh region. I then created another map using the lonboard library to display the car accidents only in that region"
      ],
      "metadata": {
        "id": "1NEHrmgMbU8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import box  # imports library\n",
        "\n",
        "\n",
        "ge_bbox = [-4.46, 55.72, -3.00, 55.97]  # defines bounding box\n",
        "gdf_accidents_ge = gdf_accidents[gdf_accidents.intersects(box(*ge_bbox))]  # flters accidents that intersect within this bounding box\n",
        "lb.viz(gdf_accidents_ge)  # maps the accident points"
      ],
      "metadata": {
        "id": "sMFqhniFb8AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B: K-means Clustering Implementation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "87v84pX7coNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Implement Kmeans clustering\n",
        "\n",
        "I implemented K-means clustering with different values of k to the filtered dataset that I created for the Glasgow-Edinburgh region."
      ],
      "metadata": {
        "id": "AJa5YOvQctup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = gdf_accidents_ge[['Longitude', 'Latitude']]  # X becomes the latitude and longitude values for the geo data frame\n",
        "kmeans3 = KMeans(n_clusters=3, random_state=42)\n",
        "gdf_accidents_ge[\"cluster_3\"] = kmeans3.fit_predict(X)  # implements kmeans clustering for 3 clusters\n",
        "\n",
        "kmeans5 = KMeans(n_clusters=5, random_state=42)\n",
        "gdf_accidents_ge[\"cluster_5\"] = kmeans5.fit_predict(X)  # implements kmeans clustering for 3 clusters"
      ],
      "metadata": {
        "id": "R_5cVa5pdwB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Mapping Clusters\n",
        "\n",
        "I mapped the clusters using the lonboard library"
      ],
      "metadata": {
        "id": "2MIUnZ5Zd0wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lonboard import Map, ScatterplotLayer, viz  # imports libraries\n",
        "\n",
        "lb.viz(gdf_accidents_ge[[\"geometry\", \"cluster_3\"]])  # maps 3 clusters for the geo data frame"
      ],
      "metadata": {
        "id": "aWi3LlE5d-Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb.viz(gdf_accidents_ge[[\"geometry\", \"cluster_5\"]])  # maps 5 clusters for the geo data frame"
      ],
      "metadata": {
        "id": "Yjv-8XBbfO96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Describe the Clustering Results\n"
      ],
      "metadata": {
        "id": "tmBJSZxOeB4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As k determines the amount of clusters that will be produced, an increase in k add more detail to the cluster map. For example changing k from 3 to 5 will show more specified correlations as it's essentially finding neighbouring points on a smaller scale."
      ],
      "metadata": {
        "id": "viR18s8LeVJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Kmeans Clustering with Attributes\n",
        "\n",
        "I implemented kmeans clustering again but this time I included the data frames \"Accident_Severity\" and \"Number_of_Vehicles"
      ],
      "metadata": {
        "id": "5utJLAkkedqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Factors = gdf_accidents_ge[[\"Accident_Severity\", \"Number_of_Vehicles\"]]  # inputs attributes for clustering\n",
        "\n",
        "kmeans3_factors = KMeans(n_clusters=3, random_state=42)\n",
        "gdf_accidents_ge[\"cluster_3_factors\"] = kmeans3_factors.fit_predict(X_Factors)  # implements kmeans clustering for 3 clusters based on the attributes provided\n",
        "\n",
        "kmeans5_factors = KMeans(n_clusters=5, random_state=42)\n",
        "gdf_accidents_ge[\"cluster_5_factors\"] = kmeans5_factors.fit_predict(X_Factors)  # implements kmeans clustering for 5 clusters based on the attributes provided"
      ],
      "metadata": {
        "id": "bfoJd6LcfE5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Visualise the results\n",
        "\n",
        "I mapped the results using the lonboard library"
      ],
      "metadata": {
        "id": "Kd02XJqmfzlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lonboard import Map, ScatterplotLayer, viz  # imports library\n",
        "\n",
        "lb.viz(gdf_accidents_ge[[\"geometry\", \"cluster_3_factors\"]])  # maps the accident points for 3 clusters in the geo data frame based on the attributes provided"
      ],
      "metadata": {
        "id": "aD2x5-VOf_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lonboard import Map, ScatterplotLayer, viz  # imports library\n",
        "\n",
        "lb.viz(gdf_accidents_ge[[\"geometry\", \"cluster_5_factors\"]])  # maps the accident points for 5 clusters in the geo data frame based on the attributes provided"
      ],
      "metadata": {
        "id": "8NaT9cCngW_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Comparison between coordinate and attribute clusters"
      ],
      "metadata": {
        "id": "vS87A7GCgnF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously stated, the cluster map with only coordinates is showing spatial patterns about where accidents are taking place. On the other hand, the cluster map that includes attributes is showing spatial patterns of what kinds of accidents are happening where. So by increasing k for the cluster map with attributes, in detail clustering of accident type can allow for analysis as to why certain accident types are occuring in a specific region"
      ],
      "metadata": {
        "id": "qoRcSM9Ugxnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C: Spatial Analysis and DBSCAN Clustering (Spatial Correlation)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rUrJMvVBhdpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Create new geopandas dataframe\n",
        "\n",
        "I created another GeoPandas Dataframe by rereading the data to avoid any confusion with the previous geodataframe. I used this dataframe to begin with DBSCAN clustering"
      ],
      "metadata": {
        "id": "cqZGlQbjhml7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_accidents = pd.read_csv(\"/content/drive/MyDrive/UK_Accident.csv\")  # loads csv file using pandas\n",
        "necessary_columns = [\"Accident_Severity\", \"Number_of_Vehicles\", \"Number_of_Casualties\", \"Speed_limit\", \"Day_of_Week\", \"Road_Type\", \"Light_Conditions\", \"Weather_Conditions\", \"Road_Surface_Conditions\", \"Urban_or_Rural_Area\", \"Year\", \"Latitude\", \"Longitude\"]\n",
        "car_accidents = car_accidents[necessary_columns]  # filters the selected columns into a subset of the dataset\n",
        "\n",
        "geometry = gpd.points_from_xy(car_accidents[\"Longitude\"], car_accidents[\"Latitude\"])  # changes latitude and longitude into geometry points\n",
        "gdf_accidents_dbscan = gpd.GeoDataFrame(car_accidents, geometry=geometry, crs=\"EPSG:4326\")  # geo data frame made from geometry points and using \"EPSG:4326\" as the CRS\n",
        "print(gdf_accidents_dbscan.head())  # shows 1st 5 rows in the dataset"
      ],
      "metadata": {
        "id": "srq3cmvOiVJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Find Necessary coordinates\n",
        "\n",
        "I used the BBox website to filter the new geodataframe to contain only the accidents around Birmingham"
      ],
      "metadata": {
        "id": "To9-HLzuig5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import box  # imports library\n",
        "\n",
        "\n",
        "bh_bbox = [-2.05, 52.35, -1.75, 52.55]  # defines bounding box\n",
        "gdf_accidents_bh = gdf_accidents[gdf_accidents.intersects(box(*bh_bbox))]  # filters accidents that intersect within this bounding box"
      ],
      "metadata": {
        "id": "9doXvfE2i5TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Map the dataset\n",
        "\n",
        "I used the lonboard library to map the Birmingham dataset"
      ],
      "metadata": {
        "id": "WR32XQ7djRJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lb.viz(gdf_accidents_bh)  # maps accident points for the geo data frame"
      ],
      "metadata": {
        "id": "W1oZBbPijdwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Identify datatypes in the dataset\n",
        "\n",
        "I used .dtypes to see which attributes were categorical and which ones were numerical which helped with calculating correlations"
      ],
      "metadata": {
        "id": "h7UNr4jcjyL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gdf_accidents_bh.dtypes)  # shows data types of all the attributes"
      ],
      "metadata": {
        "id": "OpxtY4APkIPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Correlating Numerical Attributes\n",
        "\n",
        "I ran a correlation for the numerical attributes in my dataset"
      ],
      "metadata": {
        "id": "D4ZpV0egkPbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = gdf_accidents_bh.corr(numeric_only=True)  # runs a correlation, but only for numerical attributes\n",
        "print(corr)  # prints the correlation results"
      ],
      "metadata": {
        "id": "4cnP3Hsgko_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Correlation Heatmap\n",
        "\n",
        "I adjusted a given code by adding in my dataset to create a heatmap of correlation values for the numerical attributes. The code is as follows..."
      ],
      "metadata": {
        "id": "7Uj5BYR3k7m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    fmt='.2f',           # Format annotations to 2 decimal places\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title('Pearson -Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "APQPxTNSoZLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Install Libraries\n",
        "\n",
        "I installed the pysal library to aid my analysis"
      ],
      "metadata": {
        "id": "VBqsJp6sofLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pysal  # import library"
      ],
      "metadata": {
        "id": "Jf5dmES3pBx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Install more Libraries\n",
        "\n",
        "I imported additional libraries to aid my analysis"
      ],
      "metadata": {
        "id": "ZevVved8pQP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import necessary libraries for analysis\n",
        "\n",
        "import libpysal.weights as weights\n",
        "from esda.moran import Moran"
      ],
      "metadata": {
        "id": "x2i46xj5pehW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Reprojecting the dataset\n",
        "\n",
        "I reprojected the dataset so that it alligned with the UK for spatial analysis"
      ],
      "metadata": {
        "id": "T0Tv2SlWpkLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_accidents_bh = gdf_accidents_bh.to_crs(epsg=27700)  # converts crs of geo data frame into epsg=27700\n",
        "print(gdf_accidents_bh.crs)  # we can see that it has now changed"
      ],
      "metadata": {
        "id": "7RVp2oyEpzO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10: Running Local Moran's I and Spatial clustering DBSCAN\n",
        "\n",
        "I adjusted a given code by adding in my variables and datasets in order to find out Morans I statistic value and also p-values for statistical analysis. The code is as follows..."
      ],
      "metadata": {
        "id": "v_CU7Th-p5a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = weights.DistanceBand.from_dataframe(gdf_accidents_bh, threshold=500, ids=gdf_accidents_bh.index) #Adjust this line to match your variables.\n",
        "w.transform = 'R'\n",
        "moran = Moran(gdf_accidents_bh['Accident_Severity'], w) #Adjust this line to match your variables.\n",
        "\n",
        "print(f\"\\n--- Moran's I Spatial Autocorrelation Analysis ---\")\n",
        "print(f\"Defined {w.n} observations and {w.mean_neighbors:.2f} average neighbors per point.\")\n",
        "print(f\"\\nMoran's I Statistic (Observed I): {moran.I:.4f}\")\n",
        "print(f\"P-value (significance): {moran.p_sim:.4f}\")"
      ],
      "metadata": {
        "id": "UHDbjoUaqtwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11: Describe the Results of the Correlation Analysis"
      ],
      "metadata": {
        "id": "2k5whWqYq0rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As Moran's I statistic value of -0.0057 is very close to zero, I think it can be concluded that accident severity follows a random spatial pattern. Also a p value of 0.2950 is more than the threshold of 0.05, therefore concluding that the results are not statistically significant. Ultimately, there is no spatial correlation for accident severity."
      ],
      "metadata": {
        "id": "rsGgzTLhrEdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D: DBSCAN Clustering Implementation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OKHa7zXZrrNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Implementing DBSCAN Clustering\n",
        "\n",
        "I implemented DBSCAN clustering with different eps and min_samples to the projected dataset."
      ],
      "metadata": {
        "id": "5xB1PLuxrzKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdf_accidents_bh['X'] = gdf_accidents_bh.geometry.x  # turns geometry points into numerical values\n",
        "gdf_accidents_bh['Y'] = gdf_accidents_bh.geometry.y\n",
        "\n",
        "dbscan = DBSCAN(eps=500, min_samples=10)  # eps(neigbour radius at 500m) and min_samples is 10 (number of points within radius to identify it as a cluster)\n",
        "gdf_accidents_bh[\"cluster_dbscan\"] = dbscan.fit_predict(gdf_accidents_bh[['X', 'Y']])  # implements DBSCAN clustering for the latitude and longitude of the geo data frame\n",
        "\n",
        "print(gdf_accidents_bh[['X', 'Y', 'cluster_dbscan']].head())  # shows DBSCAN clustering results"
      ],
      "metadata": {
        "id": "cGHgq6fjsEGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=500, min_samples=2)  # decreasing min_samples will make it easier for clusters to form as threshold is lower\n",
        "gdf_accidents_bh[\"cluster_dbscan\"] = dbscan.fit_predict(gdf_accidents_bh[['X', 'Y']])\n",
        "\n",
        "print(gdf_accidents_bh[['X', 'Y', 'cluster_dbscan']].head())"
      ],
      "metadata": {
        "id": "QRxVrPMKseot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=500, min_samples=5)  # changes in eps will either hone in on a cluster or demonstrate a large spatial pattern based on whether the neighbour radius is lower or higher\n",
        "gdf_accidents_bh[\"cluster_dbscan\"] = dbscan.fit_predict(gdf_accidents_bh[['X', 'Y']])\n",
        "\n",
        "print(gdf_accidents_bh[['X', 'Y', 'cluster_dbscan']].head())"
      ],
      "metadata": {
        "id": "7Y8ScSrUsg9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=500, min_samples=10)\n",
        "gdf_accidents_bh[\"cluster_dbscan\"] = dbscan.fit_predict(gdf_accidents_bh[['X', 'Y']])\n",
        "\n",
        "print(gdf_accidents_bh[['X', 'Y', 'cluster_dbscan']].head())"
      ],
      "metadata": {
        "id": "5hoG164fsjnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=500, min_samples=20)\n",
        "gdf_accidents_bh[\"cluster_dbscan\"] = dbscan.fit_predict(gdf_accidents_bh[['X', 'Y']])\n",
        "\n",
        "print(gdf_accidents_bh[['X', 'Y', 'cluster_dbscan']].head())"
      ],
      "metadata": {
        "id": "O94V_tXrsl1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Mapping Clusters\n",
        "\n",
        "I mapped the clusters using the plotly library"
      ],
      "metadata": {
        "id": "QBLbVpFIs3lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px  # imports library\n",
        "\n",
        "fig_dbscan = px.scatter_mapbox(\n",
        "    gdf_accidents_bh,  # geo data frame is the input for the map\n",
        "    lat=\"Latitude\",\n",
        "    lon=\"Longitude\",\n",
        "    color=\"cluster_dbscan\",  # colour points based of DBSCAN cluster assignment\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    zoom=9,\n",
        "    title=\"DBSCAN Clustering\",  # map title\n",
        "    height=700,\n",
        "    opacity=0.5,\n",
        "\n",
        ")\n",
        "fig_dbscan.show()"
      ],
      "metadata": {
        "id": "pXbGYPeXtGKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Describe the Culster Results"
      ],
      "metadata": {
        "id": "Rk616WNytXMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From different clustering results, I can see that more values are identified as noise when eps is lower because it means that less point meet neighbour requirement. Similarly, as min_samples increase, the number of values identified as noise increases as areas would need to be very dense in order for clusters to form."
      ],
      "metadata": {
        "id": "yrpNY4IztgGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Comparison between kmeans and DBSCAN clusters"
      ],
      "metadata": {
        "id": "rKsfMnV8tj7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kmeans clusters allowed me to preset the number of clusters that would created, therefore allowing me to control the analysis I was conducting. This in some way made it simpler and was good for seeing overall patterns. The DBSCAN clusters was good because density could be found without having to input the number of clusters. On the other hand, inputting eps and min_samples meant that certain parameters would work better than others and this could be a problem is certain parameters are left out."
      ],
      "metadata": {
        "id": "rHHXL9GstwHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: What are the real-world implications of identified clusters in the field of urban planning?"
      ],
      "metadata": {
        "id": "OaM43C4Nt0MO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of road accidents, identifying clusters means that certain regions can be highlighted for investigation and lead to possible changes in infrastructure to try and decrease accident levels. Areas can therefore be prioritized based on the clusters and even though the reason for the accidents are not included, it gives a spatial overview as to where change is required."
      ],
      "metadata": {
        "id": "MSdwdtJ-uDGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Challenge**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_6P7qm2QuZ-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Importing Libraries\n",
        "\n",
        "I imported the pandas library in order to aid my analysis"
      ],
      "metadata": {
        "id": "sIz3eJk3uudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # imporrts library"
      ],
      "metadata": {
        "id": "s9whBcVGu4ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Loading the Data\n",
        "\n",
        "I mounted my google drive to my google colab"
      ],
      "metadata": {
        "id": "0t257Zp8u6xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')  # mount the content from my google drive into my google colab notebook"
      ],
      "metadata": {
        "id": "O-ln_iJFvIjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Reading in the Data\n",
        "\n",
        "I read in the data using pandas"
      ],
      "metadata": {
        "id": "Grp6eoyrvVR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/world_cities.csv')  # load the csv file using pandas"
      ],
      "metadata": {
        "id": "DswOhcgKvl7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()  # shows the 1st 5 rows of the dataset"
      ],
      "metadata": {
        "id": "A5RP5fy2v3x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: Create \"pop_M\" column\n",
        "\n",
        "I created a new column called \"pop_M\" that showed the population in millions. To do so I divided the \"pop\" column by 1000000"
      ],
      "metadata": {
        "id": "2P2Vnqecv7yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"pop_M\"] = df[\"pop\"] / 1000000  # creates pop_M column by divided pop column by 1000000"
      ],
      "metadata": {
        "id": "Odoyzu6hweb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()  # We can see that pop_M has been added"
      ],
      "metadata": {
        "id": "jQib8kFQwhKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: Remove \"pop\" column\n",
        "\n",
        "I removed the population column from the dataset so that \"pop_M\" was left"
      ],
      "metadata": {
        "id": "VjKBULM1wjve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"pop\"])  # removes the column \"pop\" from the dataframe"
      ],
      "metadata": {
        "id": "U4MGErycwzCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()  # we can see that \"pop\" is no longer a column in the dataframe"
      ],
      "metadata": {
        "id": "7VpOLHkpw2jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6: Subset a specific city\n",
        "\n",
        "I subsetted I city that begins with the same letter as my name (Brodie,B). I chose the city Bucharest"
      ],
      "metadata": {
        "id": "AnyoeBUlw5s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucharest = df.query('city == \"Bucharest\"')  # selects Bucharest in the \"city\" column"
      ],
      "metadata": {
        "id": "U9nU0nyHx3gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7: Subset the 5 biggest cities in terms of population in the country that Bucharest belongs to (Romania)\n",
        "\n",
        "This was done in 2 steps. 1st I returned the countries column in the Bucharest dataframe to get Romania. As shown below..."
      ],
      "metadata": {
        "id": "KuQY-W6lyvX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "country = bucharest[\"country\"]  # selects country value from Bucharest subset"
      ],
      "metadata": {
        "id": "bvy7-GU5ziFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I wrote a script that would query the top 5 highest population cities for the \"pop_M\" column that alligned with the country of Romania"
      ],
      "metadata": {
        "id": "9kNeJ-D6zlbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top5_romania = df.query('country == \"Romania\"').nlargest(5, \"pop_M\")  # selects 5 largest cities in romania from the \"pop_M\" column based on population"
      ],
      "metadata": {
        "id": "1rbNZAbk0FRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then printed the results"
      ],
      "metadata": {
        "id": "t8f3AbbE0PXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(top5_romania)  # prints results to show 5 most populated cities in Romania"
      ],
      "metadata": {
        "id": "oK7t0HrE0Rs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}